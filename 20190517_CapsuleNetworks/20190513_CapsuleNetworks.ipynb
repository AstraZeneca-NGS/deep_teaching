{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample capsule neural network architecture from [Sabour, Frosst, Hinton 2017](https://arxiv.org/pdf/1710.09829.pdf)\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1500/1*AjRyyzttFIoMRb73Jzycog.png\"></img>\n",
    "[image source](https://pechyonkin.me/capsules-1/)\n",
    "\n",
    "In this notebook we'll do the following\n",
    "* build a capsule network (CapsNet) with dynamic routing\n",
    "* compare its performance in a 'small data' domain against a traditional convolutional neural network (CNN)\n",
    "* visualize its weights to understand its learned 'representation'\n",
    "* visualize how a given sample gets transformed by the capsule network\n",
    "* visualize the learned latent representation of the digit capsules ('digitcaps') \n",
    "\n",
    "Be sure to change your [Google colab](https://colab.research.google.com) runtime type by clicking `Runtime`, `Change runtime type`, and then selecting `GPU`. We'll try our luck again to see if Google let's us all get free GPUs.\n",
    "\n",
    "This notebook's Capsule Network implementations were adapted from the following sources\n",
    "* https://github.com/XifengGuo/CapsNet-Keras\n",
    "* https://keras.io/examples/cifar10_cnn_capsule/ (note that this implementation is not quite right / not quite matching the original paper)\n",
    "\n",
    "<br><br><br>\n",
    "Python / notebook cheatsheet\n",
    "* `Shift+Enter` to execute a given cell and move to / make a new next cell\n",
    "* `#` is for single line comments\n",
    "* each cell in this notebook can be either of type `code` (the cell below) or type `markdown` (this cell)\n",
    "* tabs (indents) are required for any flow control (`for`, `if..else`, `while` etc)\n",
    "* `=` is for assignment\n",
    "* `()` is to call a function or method\n",
    "* `[]` is for indexing into a variable\n",
    "* a quick way to post-hoc to put parenthesis or brackets around something, just select that something and type `(` or `[`\n",
    "* get help for any function by typing `?` after a given function (and execute the cell), or pressing `Shift Tab` when your line cursor is on the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras-specific modules we need\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from keras.initializers import RandomUniform, Zeros, Ones, glorot_uniform\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# more packages / functions that we'll need\n",
    "from IPython.core.pylabtools import figsize\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first load and visualize our sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in keras datasets: https://keras.io/datasets/\n",
    "# load a dataset of choice\n",
    "# recall each of these has 10 classes\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "#(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "#(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# shuffle the training dataset\n",
    "indices = np.arange(0,x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices,:,:]\n",
    "y_train = y_train[indices]\n",
    "\n",
    "# one-hot encode labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# add a dummy 4th dim for channel\n",
    "if len(x_train.shape)<4: \n",
    "    x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],x_train.shape[2],1))\n",
    "    x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],x_test.shape[2],1))    \n",
    "    \n",
    "# rescale and normalize\n",
    "x_train = (x_train/255.-0.5)*2\n",
    "x_test = (x_test/255.-0.5)*2\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data dimensions: n samples, by n pix, by n pix, by n channels\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize sample images from input dataset\n",
    "figsize(5,5)\n",
    "for a in range(0,36):\n",
    "    plt.subplot(6,6,a+1)\n",
    "    plt.imshow(np.squeeze(x_train[a,:,:,:]),cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in comparing the performance of Capsule networks and convolutional neural networks in 'small' data domains, so let's pull out a small subset of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get balanced subset of training and test data (to simulate small data scenario)\n",
    "nSamp = 10\n",
    "nClass = y_train.shape[1]\n",
    "x_train_subset = np.zeros((nSamp*nClass,x_train.shape[1],x_train.shape[2],x_train.shape[3]),dtype='float32')\n",
    "y_train_subset = np.zeros((nSamp*nClass,nClass),dtype='float32')\n",
    "x_test_subset = np.zeros((nSamp*nClass,x_train.shape[1],x_train.shape[2],x_train.shape[3]),dtype='float32')\n",
    "y_test_subset = np.zeros((nSamp*nClass,nClass),dtype='float32')\n",
    "for a in range(nClass):\n",
    "    CurrClassIdxs = np.nonzero(y_train[:,a])\n",
    "    CurrClassIdxs = CurrClassIdxs[0][:nSamp] # take first nSamp examples\n",
    "    CurrClassIdxs_test = np.nonzero(y_test[:,a])\n",
    "    CurrClassIdxs_test = CurrClassIdxs_test[0][:nSamp] # take first nSamp examples    \n",
    "    x_train_subset[nSamp*a:nSamp*a+nSamp,:,:,:] = x_train[CurrClassIdxs,:,:,:]\n",
    "    y_train_subset[nSamp*a:nSamp*a+nSamp,a] = y_train[CurrClassIdxs,a]\n",
    "    x_test_subset[nSamp*a:nSamp*a+nSamp,:,:,:] = x_test[CurrClassIdxs_test,:,:,:]\n",
    "    y_test_subset[nSamp*a:nSamp*a+nSamp,a] = y_test[CurrClassIdxs_test,a]\n",
    "    \n",
    "# should all equal nSamp\n",
    "np.sum(y_train_subset,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = y_train.shape[1] # =10 classes\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-code support functions for our Capsule Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Capsule Networks are similar to normal neural networks in that they \n",
    "1. take some input, \n",
    "2. transform it by multiplying it by some learned parameters (weights) and \n",
    "3. pass the resulting value(s) through another function, most often a nonlinear 'activation' function\n",
    "\n",
    "`Input ---> Input*Weights ---> f(Input*Weights)`\n",
    "\n",
    "where `f` could be anything from, e.g. `mx+b` (line; not ever used) to `max(0,x)` (relu) to `1/(1+e^-x)` (logistic function)\n",
    "\n",
    "Recall also that in order to train our neural network, we need to additionally \n",
    "4. define a performance metric, i.e. a `loss` function, with which we can compute how well our network has performed and backpropagate an error signal to update our network parameters\n",
    "\n",
    "And so we need to define for our Capsule network the following\n",
    "* a capsule 'layer' (to satisfy #2 above)\n",
    "* the capsule 'squashing' activation function (to satisfy #3 above)\n",
    "* the capsule `loss` function (to satisfy #4 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loss function'''\n",
    "# define the margin loss like hinge loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n",
    "\n",
    "'''Activation function'''\n",
    "# the squashing function.\n",
    "# we use 0.5 instead of 1 in hinton's paper.\n",
    "# if 1, the norm of vector will be zoomed out.\n",
    "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
    "# and be zoomed out while original norm is greater than 0.5.\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "# define our own softmax function instead of K.softmax\n",
    "# because K.softmax can not specify axis.\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Capsule layer'''\n",
    "class CapsuleLayer(Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        inputs_expand = K.expand_dims(inputs, 1)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
    "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
    "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
    "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
    "\n",
    "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
    "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
    "\n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "            #c = tf.nn.softmax(b, dim=1)\n",
    "            c = softmax(b, 1)\n",
    "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
    "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "            # The first two dimensions as `batch` dimension,\n",
    "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
    "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
    "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
    "\n",
    "            if i < self.routings - 1:\n",
    "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
    "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
    "                # The first two dimensions as `batch` dimension,\n",
    "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
    "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
    "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
    "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_capsule': self.num_capsule,\n",
    "            'dim_capsule': self.dim_capsule,\n",
    "            'routings': self.routings\n",
    "        }\n",
    "        base_config = super(CapsuleLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our Capsule Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the ingredients for capsule layers, let's build a Capsule Network similar to that in [Sabour, Frosst, Hinton 2017](https://arxiv.org/pdf/1710.09829.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using xinfeng guo implementation\n",
    "def makeCapsNN():\n",
    "    '''\n",
    "    In contrast to our previous tutorials, we'll use Keras' `Model` API as opposed to its `Sequential` API\n",
    "    Recall with the `Sequential` API, we used successive `.add()` calls to add layers to our model\n",
    "    With the (poorly named) `Model` or 'functional' API, you instead pass each layer as an argument to the\n",
    "    next layer\n",
    "    '''\n",
    "    input_image = Input(shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]),name='capsNN-input-image')\n",
    "    x = Conv2D(filters=32, kernel_size=(9, 9), strides=(1,1),activation='relu',name='capsNN-conv1')(input_image)\n",
    "    x = Conv2D(filters=8*2, kernel_size=(9, 9), strides=(2,2),name='primarycaps')(x) # two (9x9) convolutional capsules with 8 dims\n",
    "    x = Reshape((-1,8),name='primarycaps-reshape')(x) \n",
    "    x = Lambda(squash,name='primarycaps-squash')(x)\n",
    "    x = Dropout(0.3,name='capsNN-dropout')(x)\n",
    "    x = CapsuleLayer(num_capsule=num_classes, dim_capsule=16, routings=3,name='digitcaps')(x)\n",
    "    output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)),name='capsNN-pred-out')(x)\n",
    "    model = Model(inputs=input_image, outputs=output)\n",
    "\n",
    "    # we use the margin loss function defined earlier\n",
    "    model.compile(loss=margin_loss, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback functions to store weights from our convolutional layers after each epoch\n",
    "layer1weights_capsNN = [] # empty list to hold our weights\n",
    "layer2weights_capsNN = []\n",
    "get_weights1_capsNN = LambdaCallback(on_epoch_end=lambda epoch,logs: layer1weights_capsNN.append(myCapsNN.layers[1].get_weights())) \n",
    "get_weights2_capsNN = LambdaCallback(on_epoch_end=lambda epoch,logs: layer2weights_capsNN.append(myCapsNN.layers[2].get_weights()))\n",
    "\n",
    "myCapsNN = makeCapsNN() # make our model\n",
    "myCapsNN.summary() # print a summary of model layers, outputs shapes, and trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the numbers of parameters make sense? \n",
    "* `capsNN-conv1`: `filters=32` of `(9,9)`, so that's `32*9*9+32 = 2624`\n",
    "* `primarycaps`: `filters=8*2` of `(9,9)`, and for each of the input `32` channels from the prev layer, so that's `32*8*2*9*9+16 = 41488`\n",
    "* Now for the digitcaps, it's a bit more opaque\n",
    "    * each `primarycaps` is 8-dimensional\n",
    "    * in aggregate, our `primarycaps` outputs a `(6,6,16)` filtered 'image' i.e. `(6,6,8,2)`\n",
    "    * `primarycaps-reshape` yielded shape of `(72,8)`, i.e. we 'flatten' our `primarycaps` output into 72 vectors\n",
    "    * so technically, our two `(9x9)` convolutional capsules (with 8 dimensions) end up being 72 capsules with 8 dimensions each\n",
    "    * for each of the 72 capsules, we need to learn a weight matrix, `W` of size `(i,j)` where `i` is the number of `primarycaps` dimensions (8) and `j` is the number of `digitcaps` dimensions (16)\n",
    "        * and we need to do this for each of the digit classes (10 in total)\n",
    "    * so the parameters then are: `72*8*16*10 = 92160`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our convolutional neural network (CNN)\n",
    "We'll try our best to match the architecture and number of parameters to make a fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCNN():\n",
    "    myCNN = Sequential() # instantiate a `Sequential` model\n",
    "    myCNN.add(Conv2D(32,(9,9),activation='relu', input_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]),name='CNN-conv1'))\n",
    "    myCNN.add(Conv2D(8*2,(9,9),strides=(2,2),activation='relu',name='CNN-conv2'))\n",
    "    myCNN.add(Flatten(name='CNN-flatten'))\n",
    "    myCNN.add(Dropout(0.3,name='CNN-dropout'))    \n",
    "    myCNN.add(Dense(num_classes*16))\n",
    "    myCNN.add(Dense(num_classes,activation='softmax',name='CNN-pred-out'))\n",
    "    myCNN.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    return myCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# callback functions to store weights from our convolutional layers after each epoch\n",
    "layer1weights_cnn = [] # empty list to hold our weights\n",
    "layer2weights_cnn = []\n",
    "get_weights1_cnn = LambdaCallback(on_epoch_end=lambda epoch,logs: layer1weights_cnn.append(myCNN.layers[0].get_weights())) \n",
    "get_weights2_cnn = LambdaCallback(on_epoch_end=lambda epoch,logs: layer2weights_cnn.append(myCNN.layers[1].get_weights()))\n",
    "\n",
    "myCNN = makeCNN() # make our model\n",
    "myCNN.summary() # print a summary of model layers, outputs shapes, and trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Capsule network and the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First train the capsule network\n",
    "history_caps = myCapsNN.fit(x_train_subset,y_train_subset,batch_size=batch_size,\n",
    "                          epochs=epochs*2,validation_data=(x_test_subset, y_test_subset),\n",
    "                          callbacks=[get_weights1_capsNN,get_weights2_capsNN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_cnn = myCNN.fit(x_train_subset,y_train_subset,batch_size=batch_size,\n",
    "                        epochs=epochs*2,validation_data=(x_test_subset, y_test_subset),\n",
    "                        callbacks=[get_weights1_cnn,get_weights2_cnn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we've trained our networks, let's compare their performance as a function of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2,figsize=(8,8))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "ax[0,0].plot(history_caps.history['loss'],label='CapsNet')\n",
    "ax[0,0].plot(history_cnn.history['loss'],label='CNN')\n",
    "ax[0,0].set_title('Training data')\n",
    "ax[0,0].set_xlabel('Training epoch')\n",
    "ax[0,0].set_ylabel('Loss')\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot(history_caps.history['val_loss'],label='CapsNet')\n",
    "ax[1,0].plot(history_cnn.history['val_loss'],label='CNN')\n",
    "ax[1,0].set_title('Test data')\n",
    "ax[1,0].set_xlabel('Training epoch')\n",
    "ax[1,0].set_ylabel('Loss')\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].plot(history_caps.history['acc'],label='CapsNet')\n",
    "ax[0,1].plot(history_cnn.history['acc'],label='CNN')\n",
    "ax[0,1].set_title('Training data')\n",
    "ax[0,1].set_xlabel('Training epoch')\n",
    "ax[0,1].set_ylabel('Accuracy')\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,1].plot(history_caps.history['val_acc'],label='CapsNet')\n",
    "ax[1,1].plot(history_cnn.history['val_acc'],label='CNN')\n",
    "ax[1,1].set_title('Test data')\n",
    "ax[1,1].set_xlabel('Training epoch')\n",
    "ax[1,1].set_ylabel('Accuracy')\n",
    "ax[1,1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both networks can fit the training data well, but note how the CNN tends to overfit (test data loss increases gradually)\n",
    "\n",
    "Note that we only gave 100 test data examples. Let's see how well the models perform across the entire test dataset (10,000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myCapsNN.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCNN.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Capsule Network performs better than the CNN in this 'small data' example.\n",
    "\n",
    "On your own, you can see how changing nSamp affects performance. You can also see how the models perform on the two other datasets (fashion-MNIST and CIFAR-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize what our networks have learned\n",
    "Let's first explore the CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot a movie of our changing weights as a function of training\n",
    "def VizLayerWeights(layerweight_by_epoch_list,figtitle,subplotshape=(8,4),frameinterval=20):\n",
    "    '''\n",
    "    Inputs \n",
    "        layerweight_by_epoch_list: list, layer weights, as collected by a lambda callback\n",
    "        figtitle: string, figure title\n",
    "        subplotshape: tuple, m by n subplots to plot (which will also dictate figure size)        \n",
    "        frameinterval: int, milliseconds between movie frames in the animated figure\n",
    "    \n",
    "    Outputs\n",
    "        a jshtml animation of your filters changing with time\n",
    "    '''\n",
    "    fig, ax = plt.subplots(subplotshape[1],subplotshape[0],figsize=subplotshape)\n",
    "    fig.suptitle(figtitle)\n",
    "    weightplots = []\n",
    "    for c in range(0,len(layerweight_by_epoch_list)):\n",
    "        currweights = layerweight_by_epoch_list[c][0] # get weights from current epoch\n",
    "        currweights = np.reshape(currweights,(currweights.shape[0],currweights.shape[1],-1)) # shove all filters into one dimension\n",
    "        Counter = 0\n",
    "        weightsubplots = []\n",
    "        for a in range(0,subplotshape[1]):\n",
    "            for b in range(0,subplotshape[0]):\n",
    "                ax[a,b].axis('off')\n",
    "                img = ax[a,b].imshow(np.squeeze(currweights[:,:,Counter]))\n",
    "                Counter += 1\n",
    "                weightsubplots.append(img) # append to list of subplots\n",
    "        weightplots.append(weightsubplots) # append to list of figure frames\n",
    "\n",
    "    anim = ArtistAnimation(fig, weightplots, interval=frameinterval) # animate with prerendered plots in `weightplots`\n",
    "    plt.close() # close the actual plotted figure\n",
    "    return HTML(anim.to_jshtml()) # ...and instead display a jshtml animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer of CapsNet\n",
    "VizLayerWeights(layer1weights_capsNN,'CapsNet: 1st convolutional layer filters by epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary capsule layers (convolutional capsules)\n",
    "VizLayerWeights(layer2weights_capsNN,'CapsNet: Primary (convolutional) capsule filters by epoch (first 32)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the CNN weights look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer of CNN\n",
    "VizLayerWeights(layer1weights_cnn,'CNN: 1st convolutional layer filters by epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second convolutional layer of CNN\n",
    "VizLayerWeights(layer2weights_cnn,'CNN: 2nd convolutional layer filters by epoch (first 32)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the both the CapsNet and CNN convolutional layers are learning more or less what we expect--some sort of edge detecting filters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's visualize the data representation in each layer\n",
    "For this, we'll output a transformed sample at a few key layers in our networks\n",
    "\n",
    "Let's see the CapsNet first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sample to visualize\n",
    "ChooseIdx = 8\n",
    "\n",
    "# note how we reuse our previous model, and thus don't need to train this new model\n",
    "capsNN_conv1 = Model(inputs=myCapsNN.input,outputs=myCapsNN.get_layer('capsNN-conv1').output)\n",
    "capsNN_primarycaps = Model(inputs=myCapsNN.input,outputs=myCapsNN.get_layer('primarycaps').output)\n",
    "capsNN_digitcaps = Model(inputs=myCapsNN.input,outputs=myCapsNN.get_layer('digitcaps').output)\n",
    "capsNN_predout = Model(inputs=myCapsNN.input,outputs=myCapsNN.get_layer('capsNN-pred-out').output)\n",
    "\n",
    "# get a prediction from the new model\n",
    "capsNN_conv1_out = capsNN_conv1.predict(np.reshape(x_test[ChooseIdx,:,:,:],(1,x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "capsNN_primarycaps_out = capsNN_primarycaps.predict(np.reshape(x_test[ChooseIdx,:,:,:],(1,x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "capsNN_digitcaps_out = capsNN_digitcaps.predict(np.reshape(x_test[ChooseIdx,:,:,:],(1,x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "capsNN_predout_out = capsNN_predout.predict(np.reshape(x_test[ChooseIdx,:,:,:],(1,x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "\n",
    "print(capsNN_conv1_out.shape)\n",
    "print(capsNN_primarycaps_out.shape)\n",
    "print(capsNN_digitcaps_out.shape)\n",
    "print(capsNN_predout_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see the example we pushed through the network\n",
    "figsize(3,3)\n",
    "plt.imshow(np.squeeze(x_test[ChooseIdx,:,:,:]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample transformed 'images' from the first conv layer\n",
    "figsize(8,4)\n",
    "plt.suptitle('Data representation at layer: capsNN-conv1')\n",
    "for a in range(32):\n",
    "    plt.subplot(4,8,a+1)\n",
    "    plt.imshow(np.squeeze(capsNN_conv1_out[:,:,:,a]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample transformed 'images' from the primary capsule layers\n",
    "figsize(8,2)\n",
    "plt.suptitle('Data representation at layer: capsNN_primarycaps')\n",
    "for a in range(16):\n",
    "    plt.subplot(2,8,a+1)\n",
    "    plt.imshow(np.squeeze(capsNN_primarycaps_out[:,:,:,a]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample transformed 'images' from the digit capsule layer\n",
    "figsize(6,6)\n",
    "plt.title('Data representation at layer: capsNN_digitcaps')\n",
    "plt.imshow(np.squeeze(capsNN_digitcaps_out))\n",
    "plt.yticks([0,1,2,3,4,5,6,7,8,9])\n",
    "plt.ylabel('number')\n",
    "plt.xlabel('capsule embedding dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample transformed 'images' from the output layer of the CapsNet\n",
    "figsize(6,6)\n",
    "plt.title('Data representation at layer: capsNN_predout')\n",
    "plt.imshow(capsNN_predout_out.T)\n",
    "plt.yticks([0,1,2,3,4,5,6,7,8,9])\n",
    "plt.ylabel('number')\n",
    "plt.xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare against the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose a sample to visualize\n",
    "ChooseIdx = 8\n",
    "\n",
    "# note how we reuse our previous model, and thus don't need to train this new model\n",
    "CNN_conv1 = Model(inputs=myCNN.input,outputs=myCNN.get_layer('CNN-conv1').output)\n",
    "CNN_conv2 = Model(inputs=myCNN.input,outputs=myCNN.get_layer('CNN-conv2').output)\n",
    "CNN_predout = Model(inputs=myCNN.input,outputs=myCNN.get_layer('CNN-pred-out').output)\n",
    "\n",
    "# get a prediction from the new model\n",
    "CNN_conv1_out = CNN_conv1.predict(np.reshape(x_test[ChooseIdx,:,:,:],(1,x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "CNN_conv2_out = CNN_conv2.predict(np.reshape(x_test[ChooseIdx,:,:,:],(1,x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "CNN_predout_out = CNN_predout.predict(np.reshape(x_test[ChooseIdx,:,:,:],(1,x_train.shape[1],x_train.shape[2],x_train.shape[3])))\n",
    "\n",
    "print(CNN_conv1_out.shape)\n",
    "print(CNN_conv2_out.shape)\n",
    "print(CNN_predout_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see the example we pushed through the network\n",
    "figsize(3,3)\n",
    "plt.imshow(np.squeeze(x_test[ChooseIdx,:,:,:]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample transformed 'images' from the first CNN convolutional layer\n",
    "figsize(8,4)\n",
    "plt.suptitle('Data representation at layer: CNN-conv1')\n",
    "for a in range(32):\n",
    "    plt.subplot(4,8,a+1)\n",
    "    plt.imshow(np.squeeze(CNN_conv1_out[:,:,:,a]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample transformed 'images' from the second CNN convolutional layer\n",
    "figsize(8,2)\n",
    "plt.suptitle('Data representation at layer: CNN-conv2')\n",
    "for a in range(16):\n",
    "    plt.subplot(2,8,a+1)\n",
    "    plt.imshow(np.squeeze(CNN_conv2_out[:,:,:,a]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we saw with the learned weights, we see that the CNN is representing the data differently than the CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample transformed 'images' from the CNN output layer\n",
    "figsize(6,6)\n",
    "plt.title('Data representation at layer: CNN_predout')\n",
    "plt.imshow(CNN_predout_out.T)\n",
    "plt.yticks([0,1,2,3,4,5,6,7,8,9])\n",
    "plt.ylabel('number')\n",
    "plt.xticks([])\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize learned latent representation of `digitcaps`\n",
    "First we need to train up a decoder network to learn how to decode the `digitcaps` layer representation `(10,16)` (10 digits, 16 'features' for each digit) into a digit image.\n",
    "\n",
    "We accomplish this by making a MLP and then attaching it to the end of our CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Helper function from Xifeng Guo to mask digitcaps that are not the current digit'''\n",
    "class Mask(Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
    "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
    "    masked Tensor.\n",
    "    For example:\n",
    "        ```\n",
    "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
    "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
    "        out = Mask()(x)  # out.shape=[8, 6]\n",
    "        # or\n",
    "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
    "        ```\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
    "            # compute lengths of capsules\n",
    "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
    "            # generate the mask which is a one-hot code.\n",
    "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
    "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
    "\n",
    "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
    "        # mask.shape=[None, num_capsule]\n",
    "        # masked.shape=[None, num_capsule * dim_capsule]\n",
    "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
    "        return masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
    "        else:  # no true label provided\n",
    "            return tuple([None, input_shape[1] * input_shape[2]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Mask, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCapsNN = makeCapsNN() # re-initialize / reset our model\n",
    "\n",
    "# Decoder network MLP\n",
    "y = Input(shape=(num_classes,))\n",
    "masked_by_y = Mask()([myCapsNN.get_layer('digitcaps').output, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "decoder_only = Sequential(name='decoder_only')\n",
    "decoder_only.add(Dense(512, activation='relu',input_dim=16*num_classes,name='capsNN-decode-dense-1'))\n",
    "decoder_only.add(Dense(1024, activation='relu',name='capsNN-decode-dense-2'))\n",
    "#decoder_only.add(Dense(np.prod((x_train.shape[1],x_train.shape[2],x_train.shape[3])), activation='sigmoid')) # Xifeng's code incorrectly used a sigmoid here; we don't necessarily need an activation function, as we're looking to re-generate the original image; this is also incompatible with rescaling images from -1 to 1 as we did in our initial iamge loading\n",
    "decoder_only.add(Dense(np.prod((x_train.shape[1],x_train.shape[2],x_train.shape[3])),name='capsNN-decode-dense-3'))\n",
    "decoder_only.add(Reshape(target_shape=(x_train.shape[1],x_train.shape[2],x_train.shape[3]), name='capsNN-decode-recon'))\n",
    "\n",
    "# Make a multi-input, multi-output model (yes you can do that! these are also to different layers!)\n",
    "# Inputs: the input to our CapsNet and the training label\n",
    "# Outputs: the output of our CapsNet and the output of the `decoder` network (masked by the input training label)\n",
    "capsnet_decoder = Model([myCapsNN.get_layer('capsNN-input-image').output, y], \n",
    "                    [myCapsNN.get_layer('capsNN-pred-out').output, decoder_only(masked_by_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "capsnet_decoder.compile(optimizer='adam',loss=[margin_loss, 'mse'],loss_weights=[1., 0.392],metrics=['accuracy'])\n",
    "\n",
    "# model architecture summary\n",
    "capsnet_decoder.summary()\n",
    "decoder_only.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the layers up through `digitcaps` are exactly the same as those of our original CapsNet. This is by design. We're linking to it.\n",
    "\n",
    "Note how the layers that follow belong then to our decoder (plus some intermediate housekeeping layers) -- these are not shown in `train_model.summary()`, but can be seen when we invoke `decoder.summary()`\n",
    "\n",
    "Let's now fit the new model. Note that we are training our model and the decoder network simultaneously. Although we could use our already trained CapsNet, it'll be better for the decoder to learn if we retrain everything from scratch (thus why we called `myCapsNN = makeCapsNN()` again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "capsnet_decoder.fit([x_train_subset, y_train_subset], [y_train_subset, x_train_subset], batch_size=batch_size, epochs=epochs*4)\n",
    "#capsnet_decoder.fit([x_train, y_train], [y_train, x_train], batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (I know, it's getting more and more complicated!), we make yet another model, `explore_latent`, similar to our `capsnet_decoder`, but this time, we add an 'input' digitcaps-like layer where we will systematically explore the digitcaps latent space (by passing our own values, as opposed to the learned values of the network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to explore learned latent digitcaps space\n",
    "masked = Mask()(myCapsNN.get_layer('digitcaps').output)  # Mask using the capsule with maximal length. For prediction\n",
    "sampled_vals = Input(shape=(num_classes, 16))\n",
    "sampled_vals_digitcaps = Add()([myCapsNN.get_layer('digitcaps').output, sampled_vals])\n",
    "masked_sampled_vals_y = Mask()([sampled_vals_digitcaps, y])\n",
    "explore_latent = Model([myCapsNN.get_layer('capsNN-input-image').output, y, sampled_vals], decoder_only(masked_sampled_vals_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample vals_to_explore for each dimension of a given digit capsule\n",
    "DigitToExplore = 5\n",
    "index = np.argmax(y_test, 1) == DigitToExplore\n",
    "number = np.random.randint(low=0, high=sum(index) - 1)\n",
    "xx, yy = x_test[index][number], y_test[index][number]\n",
    "xx, yy = np.expand_dims(xx, 0), np.expand_dims(yy, 0)\n",
    "vals_to_explore=[-0.25, -0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "x_recons = []\n",
    "vals_to_explore_y = np.zeros((1,10,16))\n",
    "for dim in range(16):\n",
    "    for r in vals_to_explore:\n",
    "        tmp = np.copy(vals_to_explore_y)\n",
    "        tmp[:,:,dim] = r\n",
    "        x_recon = explore_latent.predict([xx,yy,tmp])\n",
    "        x_recons.append(x_recon)\n",
    "        \n",
    "# plot the resulting explored latent space for a given digit and its capsule\n",
    "num = len(x_recons)\n",
    "width = len(vals_to_explore)\n",
    "height = 16\n",
    "shape = x_recons[0].shape[1:3]\n",
    "image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                 dtype=x_recons[0].dtype)\n",
    "for index, img in enumerate(x_recons):\n",
    "    i = int(index/width)\n",
    "    j = index % width\n",
    "    image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = np.squeeze(img[0,:,:,0])\n",
    "\n",
    "# each row is a latent dim, each column is stepping through latent dim space\n",
    "figsize(8,12)\n",
    "plt.imshow(image,vmin=-1,vmax=1,cmap='gray')    \n",
    "plt.axis('off')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
